{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4c239b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nabeel\\miniconda3\\envs\\hackathon\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "import io\n",
    "import requests\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from atlassian import Confluence\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from docx import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc912f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nabeel\\OneDrive\\Desktop\\Programming\\Confluence_Chatbot\\research\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b761dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"C:\\Users\\Nabeel\\OneDrive\\Desktop\\Programming\\Hackathon\\HackAIdea\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e36f1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "CONFLUENCE_URL = os.getenv(\"CONFLUENCE_HOST\")\n",
    "EMAIL = os.getenv(\"EMAIL_ID\")\n",
    "TOKEN = os.getenv(\"CONFLUENCE_API_TOKEN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca8933c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PINECONE DB CONFIGURATION\n",
    "# PINECONE_API_KEY = \"PINECONE_API_KEY\"\n",
    "index_name = \"hackathon\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# PINECONE INIT\n",
    "# ------------------------------------------------------------\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=768,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "index = pc.Index(index_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bfd6bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nabeel\\AppData\\Local\\Temp\\ipykernel_19580\\3921105677.py:6: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  return HuggingFaceEmbeddings(model_name=model_name)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# EMBEDDINGS - Using Your Model\n",
    "# ------------------------------------------------------------\n",
    "def download_embeddings():\n",
    "    model_name = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "    return HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "embeddings = download_embeddings()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# EMBED TEXT\n",
    "# ------------------------------------------------------------\n",
    "def embed_text(text: str):\n",
    "    return embeddings.embed_query(text)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# TEXT CHUNKING\n",
    "# ------------------------------------------------------------\n",
    "def chunk_text(text, chunk_size=800, overlap=200):\n",
    "    if overlap >= chunk_size:\n",
    "        raise ValueError(\"overlap must be smaller than chunk_size\")\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    length = len(text)\n",
    "\n",
    "    while start < length:\n",
    "        end = min(start + chunk_size, length)\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# CLEAN HTML â†’ TEXT (includes tables)\n",
    "# ------------------------------------------------------------\n",
    "def confluence_html_to_text(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # remove useless tags\n",
    "    for tag in soup([\"script\", \"style\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    # convert tables manually\n",
    "    for table in soup.find_all(\"table\"):\n",
    "        rows = []\n",
    "        for tr in table.find_all(\"tr\"):\n",
    "            cells = [c.get_text(strip=True) for c in tr.find_all([\"td\", \"th\"])]\n",
    "            rows.append(\" | \".join(cells))\n",
    "        table.replace_with(\"\\n\".join(rows))\n",
    "\n",
    "    text = soup.get_text(separator=\"\\n\")\n",
    "    # collapse multiple blank lines\n",
    "    return \"\\n\".join([line.strip() for line in text.splitlines() if line.strip()])\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Extract text from PDF\n",
    "# ------------------------------------------------------------\n",
    "def extract_pdf_text(file_bytes):\n",
    "    text_parts = []\n",
    "    with pdfplumber.open(io.BytesIO(file_bytes)) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            extracted = page.extract_text(layout=True)\n",
    "            text_parts.append(extracted or \"\")\n",
    "    return \"\\n\\n\".join(text_parts)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Extract text from Excel (xlsx/xls)\n",
    "# ------------------------------------------------------------\n",
    "def extract_excel_text(file_bytes):\n",
    "    dfs = pd.read_excel(io.BytesIO(file_bytes), sheet_name=None)\n",
    "    output = []\n",
    "    for sheet_name, df in dfs.items():\n",
    "        output.append(f\"Sheet: {sheet_name}\")\n",
    "        output.append(df.to_string(index=False))\n",
    "    return \"\\n\\n\".join(output)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# DOWNLOAD FILE FROM URL\n",
    "# ------------------------------------------------------------\n",
    "def download_file_from_url(url):\n",
    "    \"\"\"\n",
    "    Downloads a file and returns (bytes, content_type)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        content_type = response.headers.get(\"Content-Type\", \"\").lower()\n",
    "        return response.content, content_type\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download file from {url}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Extract text from DOCX\n",
    "def extract_docx_text(file_bytes):\n",
    "    doc = Document(io.BytesIO(file_bytes))\n",
    "    text = []\n",
    "\n",
    "    for para in doc.paragraphs:\n",
    "        if para.text.strip():\n",
    "            text.append(para.text)\n",
    "\n",
    "    for table in doc.tables:\n",
    "        for row in table.rows:\n",
    "            row_data = [cell.text.strip() for cell in row.cells]\n",
    "            text.append(\" | \".join(row_data))\n",
    "\n",
    "    return \"\\n\".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb18c850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# INGEST A SINGLE PAGE + ATTACHMENTS + LINKED FILES\n",
    "# ------------------------------------------------------------\n",
    "def ingest_page(page_id):\n",
    "    print(f\"\\n=== Processing Confluence Page: {page_id} ===\\n\")\n",
    "\n",
    "    # Connect\n",
    "    confluence = Confluence(\n",
    "        url=CONFLUENCE_URL.replace(\"/wiki\", \"\"),\n",
    "        username=EMAIL,\n",
    "        password=TOKEN\n",
    "    )\n",
    "\n",
    "    # Fetch page\n",
    "    page = confluence.get_page_by_id(page_id=page_id, expand=\"body.storage\")\n",
    "    title = page[\"title\"]\n",
    "    html = page[\"body\"][\"storage\"][\"value\"]\n",
    "\n",
    "    # # Clean and chunk page text\n",
    "    # text = confluence_html_to_text(html) # extracting text using function defined above\n",
    "    # chunks = chunk_text(text) # chunking text using function defined above\n",
    "\n",
    "    # # Upload page text chunks\n",
    "    # vectors = []\n",
    "    # for i, chunk in enumerate(chunks):\n",
    "    #     vectors.append({\n",
    "    #         \"id\": f\"{page_id}-page-{i}\",\n",
    "    #         \"values\": embed_text(chunk),\n",
    "    #         \"metadata\": {\n",
    "    #             \"source\": \"page\",\n",
    "    #             \"page_id\": page_id,\n",
    "    #             \"page_title\": title,\n",
    "    #             \"page_url\": CONFLUENCE_URL + page[\"_links\"][\"webui\"],\n",
    "    #             \"chunk\": i,\n",
    "    #             \"text\": chunk\n",
    "    #         }\n",
    "    #     })\n",
    "    # index.upsert(vectors) # using upsert class present in pinecone\n",
    "    # print(f\"Uploaded {len(vectors)} page chunks for: {title}\")\n",
    "\n",
    "    # -------- Ingest Attachments --------\n",
    "    attachments = confluence.get_attachments_from_content(page_id).get(\"results\", []) # getting attachments using \"get_attachments_from_content\" class from atlassian\n",
    "    for att in attachments:\n",
    "        file_name = att[\"title\"]\n",
    "        download_link = att[\"_links\"][\"download\"]\n",
    "\n",
    "        file_bytes = requests.get(\n",
    "            CONFLUENCE_URL + download_link,\n",
    "            auth=(EMAIL, TOKEN)\n",
    "        ).content\n",
    "\n",
    "        if file_name.lower().endswith(\".pdf\"):\n",
    "            extracted = extract_pdf_text(file_bytes)\n",
    "            file_type = \"pdf\"\n",
    "        elif file_name.lower().endswith((\".xlsx\", \".xls\")):\n",
    "            extracted = extract_excel_text(file_bytes)\n",
    "            file_type = \"excel\"\n",
    "        elif file_name.lower().endswith((\".docx\", \".doc\")):\n",
    "            try:\n",
    "                import docx\n",
    "                doc = docx.Document(io.BytesIO(file_bytes))\n",
    "                extracted = \"\\n\".join([p.text for p in doc.paragraphs])\n",
    "                file_type = \"word\"\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to read Word file {file_name}: {e}\")\n",
    "                continue\n",
    "        else:\n",
    "            print(f\"Skipping unsupported attachment: {file_name}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Ingesting attachment: {file_name}\")\n",
    "\n",
    "        # file_chunks = chunk_text(extracted) # chunking text exatracted from attachements\n",
    "        # file_vectors = []\n",
    "        # for i, chunk in enumerate(file_chunks):\n",
    "        #     file_vectors.append({\n",
    "        #         \"id\": f\"{page_id}-{file_name}-{i}\",\n",
    "        #         \"values\": embed_text(chunk),\n",
    "        #         \"metadata\": {\n",
    "        #             \"source\": \"attachment\",\n",
    "        #             \"file_type\": file_type,\n",
    "        #             \"filename\": file_name,\n",
    "        #             \"page_id\": page_id,\n",
    "        #             \"page_title\": title,\n",
    "        #             \"page_url\": CONFLUENCE_URL + page[\"_links\"][\"webui\"],\n",
    "        #             \"chunk\": i,\n",
    "        #             \"text\": chunk\n",
    "        #         }\n",
    "        #     })\n",
    "        # index.upsert(file_vectors) # using upsert class present in pinecone\n",
    "        # print(f\"Uploaded {len(file_vectors)} chunks from attachment: {file_name}\")\n",
    "\n",
    "    # # -------- Ingest Linked Files in Page --------\n",
    "    # soup = BeautifulSoup(html, \"html.parser\") # converting to Document Object Model (DOM)\n",
    "    # links = [a[\"href\"] for a in soup.find_all(\"a\", href=True)] # extracting all links\n",
    "\n",
    "    # for link in links:\n",
    "    #     file_bytes, new = download_file_from_url(link)\n",
    "    #     # print(file_bytes)\n",
    "    #     if not file_bytes:\n",
    "    #         continue\n",
    "    #     print(file_bytes)\n",
    "    #     print(\"***\")\n",
    "    #     print(new)\n",
    "\n",
    "    \n",
    "    # ## Detect type based on SharePoint hint in URL\n",
    "    #     print(\"check point\")\n",
    "    #     if \":w:\" in link:\n",
    "    #         # import docx\n",
    "    #         # doc = docx.Document(io.BytesIO(file_bytes))\n",
    "    #         # extracted = \"\\n\".join([p.text for p in doc.paragraphs])\n",
    "    #         # file_type = \"word\"\n",
    "    #         extracted = extract_docx_text(file_bytes)\n",
    "    #         file_type = \"word\"\n",
    "\n",
    "    #     elif \":x:\" in link:\n",
    "    #         extracted = extract_excel_text(file_bytes)\n",
    "    #         file_type = \"excel\"\n",
    "    #     # elif link.lower().endswith(\".pdf\"):\n",
    "    #     #     extracted = extract_pdf_text(file_bytes)\n",
    "    #     #     file_type = \"pdf\"\n",
    "    #     else:\n",
    "    #         print(f\"Skipping unsupported linked file: {link}\")\n",
    "    #         continue\n",
    "\n",
    "    #     file_chunks = chunk_text(extracted) # chunking text exatracted from linked files\n",
    "    #     file_vectors = []\n",
    "    #     for i, chunk in enumerate(file_chunks):\n",
    "    #         file_vectors.append({\n",
    "    #             \"id\": f\"{page_id}-{i}\",\n",
    "    #             \"metadata\": {\n",
    "    #                 \"source\": \"linked_file\",\n",
    "    #                 \"file_type\": file_type,\n",
    "    #                 \"page_id\": page_id,\n",
    "    #                 \"page_url\": CONFLUENCE_URL + page[\"_links\"][\"webui\"],\n",
    "    #                 \"file_url\": link,\n",
    "    #                 \"chunk\": i,\n",
    "    #                 \"text\": chunk\n",
    "    #             }\n",
    "    #         })\n",
    "    #     index.upsert(file_vectors) # using upsert class present in pinecone\n",
    "    #     print(f\"Uploaded {len(file_vectors)} chunks from linked file: {link}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34d9f5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Confluence Page: 819201 ===\n",
      "\n",
      "Ingesting attachment: data_dictionary.xlsx\n",
      "Ingesting attachment: design_document.xlsx\n",
      "\n",
      "=== Processing Confluence Page: 1277953 ===\n",
      "\n",
      "\n",
      "==== ALL PAGES INGESTED ====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extracting multiple pages and Storing in Pinecone DB\n",
    "PAGE_IDS = [\"819201\", \"1277953\"]  # Add more page IDs as needed\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# INGEST MULTIPLE PAGES\n",
    "# ------------------------------------------------------------\n",
    "def ingest_multiple_pages(page_ids):\n",
    "    for page_id in page_ids:\n",
    "        try:\n",
    "            ingest_page(page_id)\n",
    "        except Exception as e:\n",
    "            print(f\"Error ingesting page {page_id}: {e}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# RUN BATCH INGESTION\n",
    "# ------------------------------------------------------------\n",
    "ingest_multiple_pages(PAGE_IDS)\n",
    "print(\"\\n==== ALL PAGES INGESTED ====\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392e8290",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
