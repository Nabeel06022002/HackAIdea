{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4c239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "import io\n",
    "import requests\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from atlassian import Confluence\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from docx import Document\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from flask import Flask, render_template, jsonify, request\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc912f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nabeel\\OneDrive\\Desktop\\Programming\\Confluence_Chatbot\\research\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b761dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"C:\\Users\\Nabeel\\OneDrive\\Desktop\\Programming\\Hackathon\\HackAIdea\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e36f1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "CONFLUENCE_URL = os.getenv(\"CONFLUENCE_HOST\")\n",
    "EMAIL = os.getenv(\"EMAIL_ID\")\n",
    "TOKEN = os.getenv(\"CONFLUENCE_API_TOKEN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca8933c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PINECONE DB CONFIGURATION\n",
    "# PINECONE_API_KEY = \"PINECONE_API_KEY\"\n",
    "index_name = \"hackathon\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# PINECONE INIT\n",
    "# ------------------------------------------------------------\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=768,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "index = pc.Index(index_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bfd6bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nabeel\\AppData\\Local\\Temp\\ipykernel_15340\\3921105677.py:6: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  return HuggingFaceEmbeddings(model_name=model_name)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# EMBEDDINGS - Using Your Model\n",
    "# ------------------------------------------------------------\n",
    "def download_embeddings():\n",
    "    model_name = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "    return HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "embeddings = download_embeddings()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# EMBED TEXT\n",
    "# ------------------------------------------------------------\n",
    "def embed_text(text: str):\n",
    "    return embeddings.embed_query(text)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# TEXT CHUNKING\n",
    "# ------------------------------------------------------------\n",
    "def chunk_text(text, chunk_size=800, overlap=200):\n",
    "    if overlap >= chunk_size:\n",
    "        raise ValueError(\"overlap must be smaller than chunk_size\")\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    length = len(text)\n",
    "\n",
    "    while start < length:\n",
    "        end = min(start + chunk_size, length)\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# CLEAN HTML → TEXT (includes tables)\n",
    "# ------------------------------------------------------------\n",
    "def confluence_html_to_text(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # remove useless tags\n",
    "    for tag in soup([\"script\", \"style\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    # convert tables manually\n",
    "    for table in soup.find_all(\"table\"):\n",
    "        rows = []\n",
    "        for tr in table.find_all(\"tr\"):\n",
    "            cells = [c.get_text(strip=True) for c in tr.find_all([\"td\", \"th\"])]\n",
    "            rows.append(\" | \".join(cells))\n",
    "        table.replace_with(\"\\n\".join(rows))\n",
    "\n",
    "    text = soup.get_text(separator=\"\\n\")\n",
    "    # collapse multiple blank lines\n",
    "    return \"\\n\".join([line.strip() for line in text.splitlines() if line.strip()])\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Extract text from PDF\n",
    "# ------------------------------------------------------------\n",
    "def extract_pdf_text(file_bytes):\n",
    "    text_parts = []\n",
    "    with pdfplumber.open(io.BytesIO(file_bytes)) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            extracted = page.extract_text(layout=True)\n",
    "            text_parts.append(extracted or \"\")\n",
    "    return \"\\n\\n\".join(text_parts)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Extract text from Excel (xlsx/xls)\n",
    "# ------------------------------------------------------------\n",
    "def extract_excel_text(file_bytes):\n",
    "    dfs = pd.read_excel(io.BytesIO(file_bytes), sheet_name=None)\n",
    "    output = []\n",
    "    for sheet_name, df in dfs.items():\n",
    "        output.append(f\"Sheet: {sheet_name}\")\n",
    "        output.append(df.to_string(index=False))\n",
    "    return \"\\n\\n\".join(output)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# DOWNLOAD FILE FROM URL\n",
    "# ------------------------------------------------------------\n",
    "def download_file_from_url(url):\n",
    "    \"\"\"\n",
    "    Downloads a file and returns (bytes, content_type)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        content_type = response.headers.get(\"Content-Type\", \"\").lower()\n",
    "        return response.content, content_type\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download file from {url}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Extract text from DOCX\n",
    "def extract_docx_text(file_bytes):\n",
    "    doc = Document(io.BytesIO(file_bytes))\n",
    "    text = []\n",
    "\n",
    "    for para in doc.paragraphs:\n",
    "        if para.text.strip():\n",
    "            text.append(para.text)\n",
    "\n",
    "    for table in doc.tables:\n",
    "        for row in table.rows:\n",
    "            row_data = [cell.text.strip() for cell in row.cells]\n",
    "            text.append(\" | \".join(row_data))\n",
    "\n",
    "    return \"\\n\".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb18c850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# INGEST A SINGLE PAGE + ATTACHMENTS + LINKED FILES\n",
    "# ------------------------------------------------------------\n",
    "def ingest_page(page_id):\n",
    "    print(f\"\\n=== Processing Confluence Page: {page_id} ===\\n\")\n",
    "\n",
    "    # Connect\n",
    "    confluence = Confluence(\n",
    "        url=CONFLUENCE_URL.replace(\"/wiki\", \"\"),\n",
    "        username=EMAIL,\n",
    "        password=TOKEN\n",
    "    )\n",
    "\n",
    "    # Fetch page\n",
    "    page = confluence.get_page_by_id(page_id=page_id, expand=\"body.storage\")\n",
    "    title = page[\"title\"]\n",
    "    html = page[\"body\"][\"storage\"][\"value\"]\n",
    "\n",
    "    # # Clean and chunk page text\n",
    "    text = confluence_html_to_text(html) # extracting text using function defined above\n",
    "    chunks = chunk_text(text) # chunking text using function defined above\n",
    "\n",
    "    # Upload page text chunks\n",
    "    vectors = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        vectors.append({\n",
    "            \"id\": f\"{page_id}-page-{i}\",\n",
    "            \"values\": embed_text(chunk),\n",
    "            \"metadata\": {\n",
    "                \"source\": \"page\",\n",
    "                \"page_id\": page_id,\n",
    "                \"page_title\": title,\n",
    "                \"page_url\": CONFLUENCE_URL + page[\"_links\"][\"webui\"],\n",
    "                \"chunk\": i,\n",
    "                \"text\": chunk\n",
    "            }\n",
    "        })\n",
    "    index.upsert(vectors) # using upsert class present in pinecone\n",
    "    print(f\"Uploaded {len(vectors)} page chunks for: {title}\")\n",
    "\n",
    "    # -------- Ingest Attachments --------\n",
    "    attachments = confluence.get_attachments_from_content(page_id).get(\"results\", []) # getting attachments using \"get_attachments_from_content\" class from atlassian\n",
    "    for att in attachments:\n",
    "        file_name = att[\"title\"]\n",
    "        download_link = att[\"_links\"][\"download\"]\n",
    "\n",
    "        file_bytes = requests.get(\n",
    "            CONFLUENCE_URL + download_link,\n",
    "            auth=(EMAIL, TOKEN)\n",
    "        ).content\n",
    "\n",
    "        if file_name.lower().endswith(\".pdf\"):\n",
    "            extracted = extract_pdf_text(file_bytes)\n",
    "            file_type = \"pdf\"\n",
    "        elif file_name.lower().endswith((\".xlsx\", \".xls\")):\n",
    "            extracted = extract_excel_text(file_bytes)\n",
    "            file_type = \"excel\"\n",
    "        elif file_name.lower().endswith((\".docx\", \".doc\")):\n",
    "            try:\n",
    "                import docx\n",
    "                doc = docx.Document(io.BytesIO(file_bytes))\n",
    "                extracted = \"\\n\".join([p.text for p in doc.paragraphs])\n",
    "                file_type = \"word\"\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to read Word file {file_name}: {e}\")\n",
    "                continue\n",
    "        else:\n",
    "            print(f\"Skipping unsupported attachment: {file_name}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Ingesting attachment: {file_name}\")\n",
    "\n",
    "        file_chunks = chunk_text(extracted) # chunking text exatracted from attachements\n",
    "        file_vectors = []\n",
    "        for i, chunk in enumerate(file_chunks):\n",
    "            file_vectors.append({\n",
    "                \"id\": f\"{page_id}-{file_name}-{i}\",\n",
    "                \"values\": embed_text(chunk),\n",
    "                \"metadata\": {\n",
    "                    \"source\": \"attachment\",\n",
    "                    \"file_type\": file_type,\n",
    "                    \"filename\": file_name,\n",
    "                    \"page_id\": page_id,\n",
    "                    \"page_title\": title,\n",
    "                    \"page_url\": CONFLUENCE_URL + page[\"_links\"][\"webui\"],\n",
    "                    \"chunk\": i,\n",
    "                    \"text\": chunk\n",
    "                }\n",
    "            })\n",
    "        index.upsert(file_vectors) # using upsert class present in pinecone\n",
    "        print(f\"Uploaded {len(file_vectors)} chunks from attachment: {file_name}\")\n",
    "\n",
    "    # # -------- Ingest Linked Files in Page --------\n",
    "    # soup = BeautifulSoup(html, \"html.parser\") # converting to Document Object Model (DOM)\n",
    "    # links = [a[\"href\"] for a in soup.find_all(\"a\", href=True)] # extracting all links\n",
    "\n",
    "    # for link in links:\n",
    "    #     file_bytes, new = download_file_from_url(link)\n",
    "    #     # print(file_bytes)\n",
    "    #     if not file_bytes:\n",
    "    #         continue\n",
    "    #     print(file_bytes)\n",
    "    #     print(\"***\")\n",
    "    #     print(new)\n",
    "\n",
    "    \n",
    "    # ## Detect type based on SharePoint hint in URL\n",
    "    #     print(\"check point\")\n",
    "    #     if \":w:\" in link:\n",
    "    #         # import docx\n",
    "    #         # doc = docx.Document(io.BytesIO(file_bytes))\n",
    "    #         # extracted = \"\\n\".join([p.text for p in doc.paragraphs])\n",
    "    #         # file_type = \"word\"\n",
    "    #         extracted = extract_docx_text(file_bytes)\n",
    "    #         file_type = \"word\"\n",
    "\n",
    "    #     elif \":x:\" in link:\n",
    "    #         extracted = extract_excel_text(file_bytes)\n",
    "    #         file_type = \"excel\"\n",
    "    #     # elif link.lower().endswith(\".pdf\"):\n",
    "    #     #     extracted = extract_pdf_text(file_bytes)\n",
    "    #     #     file_type = \"pdf\"\n",
    "    #     else:\n",
    "    #         print(f\"Skipping unsupported linked file: {link}\")\n",
    "    #         continue\n",
    "\n",
    "    #     file_chunks = chunk_text(extracted) # chunking text exatracted from linked files\n",
    "    #     file_vectors = []\n",
    "    #     for i, chunk in enumerate(file_chunks):\n",
    "    #         file_vectors.append({\n",
    "    #             \"id\": f\"{page_id}-{i}\",\n",
    "    #             \"metadata\": {\n",
    "    #                 \"source\": \"linked_file\",\n",
    "    #                 \"file_type\": file_type,\n",
    "    #                 \"page_id\": page_id,\n",
    "    #                 \"page_url\": CONFLUENCE_URL + page[\"_links\"][\"webui\"],\n",
    "    #                 \"file_url\": link,\n",
    "    #                 \"chunk\": i,\n",
    "    #                 \"text\": chunk\n",
    "    #             }\n",
    "    #         })\n",
    "    #     index.upsert(file_vectors) # using upsert class present in pinecone\n",
    "    #     print(f\"Uploaded {len(file_vectors)} chunks from linked file: {link}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34d9f5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Confluence Page: 819201 ===\n",
      "\n",
      "Uploaded 2 page chunks for: Data Asset: sample_data_table\n",
      "Ingesting attachment: data_dictionary.xlsx\n",
      "Uploaded 1 chunks from attachment: data_dictionary.xlsx\n",
      "Ingesting attachment: design_document.xlsx\n",
      "Uploaded 3 chunks from attachment: design_document.xlsx\n",
      "\n",
      "=== Processing Confluence Page: 1277953 ===\n",
      "\n",
      "Uploaded 3 page chunks for: Advancements in AI\n",
      "\n",
      "==== ALL PAGES INGESTED ====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extracting multiple pages and Storing in Pinecone DB\n",
    "PAGE_IDS = [\"819201\", \"1277953\"]  # Add more page IDs as needed\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# INGEST MULTIPLE PAGES\n",
    "# ------------------------------------------------------------\n",
    "def ingest_multiple_pages(page_ids):\n",
    "    for page_id in page_ids:\n",
    "        try:\n",
    "            ingest_page(page_id)\n",
    "        except Exception as e:\n",
    "            print(f\"Error ingesting page {page_id}: {e}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# RUN BATCH INGESTION\n",
    "# ------------------------------------------------------------\n",
    "ingest_multiple_pages(PAGE_IDS)\n",
    "print(\"\\n==== ALL PAGES INGESTED ====\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "392e8290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'data\\\\artificial-intelligence-modern-approach.9780131038059.25368.pdf', 'text': 'getting at the same goal: reducing the variance in the language model.\\nOne complication: note that the expression P(c\\ni |ci−2:i−1) asks for P(c1 |c-1:0) when\\ni =1 , but there are no characters before c1. We can introduce artiﬁcial characters, for\\nexample, deﬁning c0 to be a space character or a special “begin text” character. Or we can\\nfall back on lower-order Markov models, in effect deﬁning c-1:0 to be the empty sequence\\nand thus P(c1 |c-1:0)= P(c1).\\n22.1.3 Model evaluation\\nWith so many possible n-gram models—unigram, bigram, trigram, interpolated smoothing\\nwith different values of λ, etc.—how do we know what model to choose? We can evaluate a\\nmodel with cross-validation. Split the corpus into a training corpus and a validation corpus.'}\n"
     ]
    }
   ],
   "source": [
    "vector_id = \"000410ab-51e6-4894-8093-3f02e614fa8c\"\n",
    "result = index.fetch(ids=[vector_id])\n",
    "metadata = result.vectors[vector_id].metadata\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d17655fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"You are a knowledgeable assistant with access to internal documentation from Confluence pages. \"\n",
    "    \"Use the provided context to answer the user’s questions as accurately as possible. \"\n",
    "    \"Only use the information given in the context. If the answer is not present, say 'I don’t know'. \"\n",
    "    \"{context}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eacd5aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = download_embeddings()\n",
    "\n",
    "index_name = \"hackathon\" \n",
    "# Embed each chunk and upsert the embeddings into your Pinecone index.\n",
    "docsearch = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "585bc765",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})\n",
    "\n",
    "# chatModel = ChatOpenAI(model=\"gpt-4o\")\n",
    "chatModel = ChatOllama(model=\"llama2:latest\")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(chatModel, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3af714e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(msg):\n",
    "    # msg = request.form[\"msg\"]\n",
    "    print(\"User Input:\", msg)\n",
    "\n",
    "    response = rag_chain.invoke({\"input\": msg})\n",
    "\n",
    "    answer = response[\"answer\"]\n",
    "    docs = response[\"context\"]   # retrieved documents\n",
    "\n",
    "    citations = []\n",
    "    for doc in docs:\n",
    "        meta = doc.metadata\n",
    "\n",
    "        citations.append({\n",
    "            # \"source\": meta.get(\"source\"),\n",
    "            \"page_url\": meta.get(\"page_url\"),\n",
    "            # \"score\": meta.get(\"score\"),  # optional if stored\n",
    "        })\n",
    "\n",
    "    # return jsonify({\n",
    "    #     \"answer\": answer,\n",
    "    #     \"citations\": citations\n",
    "    # })\n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"citations\": citations\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5e63cad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: What all attributes are present in the Sample Data Asset?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer': \"Based on the information provided in the context, the following attributes are present in the Sample Data Asset:\\n\\n1. id\\n2. name\\n3. created_at\\n4. value\\n\\nTherefore, the answer to the user's question is:\\n\\nThe Sample Data Asset contains the following attributes: id, name, created_at, and value.\",\n",
       " 'citations': [{'page_url': 'https://nabeelnizam78.atlassian.net/wiki/spaces/MFS/pages/819201/Data+Asset+sample_data_table'},\n",
       "  {'page_url': None},\n",
       "  {'page_url': 'https://nabeelnizam78.atlassian.net/wiki/spaces/MFS/pages/819201/Data+Asset+sample_data_table'}]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg = \"What all attributes are present in the Sample Data Asset?\"\n",
    "chat(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0b477a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Working outside of application context.\n\nThis typically means that you attempted to use functionality that needed\nthe current application. To solve this, set up an application context\nwith app.app_context(). See the documentation for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mjsonify\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nabeel\\miniconda3\\envs\\hackathon\\lib\\site-packages\\flask\\json\\__init__.py:170\u001b[0m, in \u001b[0;36mjsonify\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mjsonify\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Response:\n\u001b[0;32m    139\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Serialize the given arguments as JSON, and return a\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :class:`~flask.Response` object with the ``application/json``\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    mimetype. A dict or list returned from a view will be converted to a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 0.2\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcurrent_app\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[38;5;241m.\u001b[39mresponse(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Nabeel\\miniconda3\\envs\\hackathon\\lib\\site-packages\\werkzeug\\local.py:318\u001b[0m, in \u001b[0;36m_ProxyLookup.__get__\u001b[1;34m(self, instance, owner)\u001b[0m\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43minstance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_current_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfallback \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Nabeel\\miniconda3\\envs\\hackathon\\lib\\site-packages\\werkzeug\\local.py:519\u001b[0m, in \u001b[0;36mLocalProxy.__init__.<locals>._get_current_object\u001b[1;34m()\u001b[0m\n\u001b[0;32m    517\u001b[0m     obj \u001b[38;5;241m=\u001b[39m local\u001b[38;5;241m.\u001b[39mget()\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(unbound_message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m get_name(obj)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Working outside of application context.\n\nThis typically means that you attempted to use functionality that needed\nthe current application. To solve this, set up an application context\nwith app.app_context(). See the documentation for more information."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
